package main

import (
	"context"
	"encoding/base64"
	"fmt"
	"io"
	"log"
	"net/http"
	"net/url"
	"regexp"
	"strings"
	"sync"
	"time"

	"earth/bpsocket"
)

// DTNJsonRequest DTNçµŒç”±ã§å—ä¿¡ã™ã‚‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ§‹é€ ä½“
type DTNJsonRequest struct {
	RequestID string              `json:"request_id"`
	Method    string              `json:"method"`
	URL       string              `json:"url"`
	Headers   map[string][]string `json:"headers"`
	Body      string              `json:"body"` // Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
	Version   int                 `json:"version"`
}

// CrawlRequest å†…éƒ¨å‡¦ç†ç”¨ã®ã‚¯ãƒ­ãƒ¼ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ§‹é€ ä½“
type CrawlRequest struct {
	RequestID string
	URL       string
	Depth     int
}

// BpResponse HTTPãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«å¿…è¦ãªæƒ…å ±ã‚’æ ¼ç´ã™ã‚‹æ§‹é€ ä½“
type BpResponse struct {
	RequestID     string              `json:"request_id"`
	StatusCode    int                 `json:"status_code"`
	Headers       map[string][]string `json:"headers"`
	Body          string              `json:"body"` // Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
	ContentType   string              `json:"content_type,omitempty"`
	ContentLength int64               `json:"content_length,omitempty"`
	Depth         int                 `json:"-"` // å†…éƒ¨ç®¡ç†ç”¨ (JSONã«ã¯å«ã‚ãªã„)
}

// å…±é€šãƒªã‚½ãƒ¼ã‚¹
var (
	visitedURLs  = make(map[string]bool)
	visitedMutex sync.Mutex
	linkRegex    = regexp.MustCompile(`(?i)<a\s+(?:[^>]*?\s+)?href=["']?([^"'>\s]+)["']?`)
	maxDepth     = 2
)

func main() {
	log.Println("=== Earth Station with BP Socket Gateway ===")

	// BP Socketè¨­å®š
	const (
		localNodeNum   = 150 // Earth node
		localSvcNum    = 1   // Receive on ipn:150.1
		sendFromSvcNum = 2   // Send from ipn:150.2
		remoteNodeNum  = 149 // Space node
		remoteSvcNum   = 1   // Send to ipn:149.1
	)

	// BP Socket Receiverã®åˆæœŸåŒ–
	receiver, err := bpsocket.NewBpReceiver(localNodeNum, localSvcNum)
	if err != nil {
		log.Fatalf("Failed to create BP receiver: %v", err)
	}
	defer receiver.Close()

	// BP Socket Senderã®åˆæœŸåŒ–
	sender, err := bpsocket.NewBpSender(localNodeNum, sendFromSvcNum, remoteNodeNum, remoteSvcNum)
	if err != nil {
		log.Fatalf("Failed to create BP sender: %v", err)
	}
	defer sender.Close()

	// ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç”¨ãƒãƒ£ãƒãƒ«ã®ä½œæˆ
	urlChan := make(chan CrawlRequest, 100)
	bpResChan := make(chan BpResponse, 100)
	sendChan := make(chan BpResponse, 100)

	var wg sync.WaitGroup

	// å—ä¿¡ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹
	receiver.Start()

	// --- 1. Recv Stage (BP Socketã‹ã‚‰é€£ç¶šå—ä¿¡) ---
	wg.Add(1)
	go func() {
		defer wg.Done()
		recvStageBpSocket(receiver.GetDataChannel(), urlChan)
	}()

	// --- 2. Fetch Stage (HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ) ---
	const fetchWorkers = 5
	for i := 0; i < fetchWorkers; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			fetchWorkerBpSocket(urlChan, bpResChan)
		}()
	}

	// --- 3. Save & Recurse Stage (å†å¸°å‡¦ç†ã¨sendChanã¸ã®è»¢é€) ---
	wg.Add(1)
	go func() {
		defer wg.Done()
		saveAndRecurseWorkerBpSocket(bpResChan, urlChan, sendChan)
	}()

	// --- 4. Send Stage (BP Socketã§é€ä¿¡) ---
	const sendWorkers = 3
	for i := 0; i < sendWorkers; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			sendWorkerBpSocket(sendChan, sender, workerID)
		}(i)
	}

	log.Println("Earth Station is running with BP Socket... (Ctrl+C to exit)")
	wg.Wait()
}

// recvStageBpSocket: BP Socketã‹ã‚‰é€£ç¶šçš„ã«ãƒãƒ³ãƒ‰ãƒ«ã‚’å—ä¿¡ã—ã¦URLã‚’æŠ½å‡º
func recvStageBpSocket(dataChan <-chan []byte, urlChan chan<- CrawlRequest) {
	for data := range dataChan {
		log.Printf(">>> Recv Stage: Received bundle (%d bytes)", len(data))

		// JSONã‚’ãƒ‘ãƒ¼ã‚¹
		targetURL, reqID, err := bpsocket.ParseDTNRequest(data)
		if err != nil {
			log.Printf("âš ï¸  Parse error: %v", err)
			// ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ
			errorURL := fmt.Sprintf("error://invalid-request/%s", url.QueryEscape(err.Error()))
			urlChan <- CrawlRequest{RequestID: reqID, URL: errorURL, Depth: 0}
			continue
		}

		log.Printf("ğŸ”„ NEW REQUEST: %s (ID: %s)", targetURL, reqID)
		urlChan <- CrawlRequest{RequestID: reqID, URL: targetURL, Depth: 0}
	}
}

// fetchWorkerBpSocket: HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®Ÿè¡Œ
func fetchWorkerBpSocket(urlChan <-chan CrawlRequest, bpResChan chan<- BpResponse) {
	client := http.Client{Timeout: 30 * time.Second}

	for reqInfo := range urlChan {
		targetURL := reqInfo.URL
		reqID := reqInfo.RequestID
		depth := reqInfo.Depth

		// ã‚¨ãƒ©ãƒ¼URLã®æ¤œå‡º
		if strings.HasPrefix(targetURL, "error://") {
			errRes := BpResponse{
				RequestID:     reqID,
				StatusCode:    400,
				Headers:       map[string][]string{"Content-Type": {"text/plain"}},
				Body:          base64.StdEncoding.EncodeToString([]byte("Error: Invalid or incomplete HTTP request")),
				ContentType:   "text/plain",
				ContentLength: int64(len("Error: Invalid or incomplete HTTP request")),
				Depth:         0,
			}
			bpResChan <- errRes
			log.Printf("âŒ Sent 400 Bad Request for: %s", targetURL)
			continue
		}

		// å†è¨ªå•ãƒã‚§ãƒƒã‚¯
		visitedMutex.Lock()
		if visitedURLs[targetURL] {
			visitedMutex.Unlock()
			continue
		}
		visitedURLs[targetURL] = true
		visitedMutex.Unlock()

		log.Printf("ğŸ•¸ï¸  Fetching: %s", targetURL)

		// HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å®Ÿè¡Œ
		req, err := http.NewRequestWithContext(context.Background(), "GET", targetURL, nil)
		if err != nil {
			log.Printf("âš ï¸  Request creation error (%s): %v", targetURL, err)
			continue
		}

		resp, err := client.Do(req)
		if err != nil {
			log.Printf("âš ï¸  HTTP request error (%s): %v", targetURL, err)
			continue
		}

		bodyBytes, err := io.ReadAll(resp.Body)
		resp.Body.Close()
		if err != nil {
			log.Printf("âš ï¸  Body read error (%s): %v", targetURL, err)
			continue
		}

		bpRes := BpResponse{
			RequestID:     reqID,
			StatusCode:    resp.StatusCode,
			Headers:       resp.Header,
			Body:          base64.StdEncoding.EncodeToString(bodyBytes),
			ContentType:   resp.Header.Get("Content-Type"),
			ContentLength: resp.ContentLength,
			Depth:         depth,
		}
		bpRes.Headers["X-Original-URL"] = []string{targetURL}

		bpResChan <- bpRes
		log.Printf("âœ… Fetched: %s (Status: %d, Size: %d bytes)", targetURL, bpRes.StatusCode, len(bodyBytes))
	}
}

// saveAndRecurseWorkerBpSocket: å†å¸°ãƒªãƒ³ã‚¯ã®å‡¦ç†ã¨sendChanã¸ã®è»¢é€
func saveAndRecurseWorkerBpSocket(bpResChan <-chan BpResponse, urlChan chan<- CrawlRequest, sendChan chan<- BpResponse) {
	for bpRes := range bpResChan {
		originalURL := bpRes.Headers["X-Original-URL"][0]

		// ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã§ã‚‚é€ä¿¡ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
		sendChan <- bpRes

		// ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆã€å†å¸°å‡¦ç†ã¯è¡Œã‚ãªã„
		if bpRes.StatusCode == 400 {
			log.Printf("âš ï¸  Skipping recursion for error response")
			continue
		}

		// å†å¸°ãƒªãƒ³ã‚¯ã®å‡¦ç†
		currentDepth := bpRes.Depth
		if currentDepth < maxDepth {
			links := extractLinksBpSocket(bpRes, originalURL)
			for _, link := range links {
				visitedMutex.Lock()
				if !visitedURLs[link] {
					visitedMutex.Unlock()
					urlChan <- CrawlRequest{RequestID: bpRes.RequestID, URL: link, Depth: currentDepth + 1}
					log.Printf("ğŸ”— Link Found (Depth %d): %s", currentDepth+1, link)
				} else {
					visitedMutex.Unlock()
				}
			}
		}
	}
	close(sendChan)
}

// sendWorkerBpSocket: BP Socketã§ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’é€ä¿¡
func sendWorkerBpSocket(bpResChan <-chan BpResponse, sender *bpsocket.BpSender, workerID int) {
	for bpRes := range bpResChan {
		log.Printf("ğŸš€ [Worker %d] Sending response (ID: %s, Status: %d)", workerID, bpRes.RequestID, bpRes.StatusCode)

		ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
		err := sender.Send(ctx, bpRes)
		cancel()

		if err != nil {
			log.Printf("âŒ [Worker %d] Send error: %v", workerID, err)
		} else {
			log.Printf("âœ… [Worker %d] Response sent successfully (ID: %s)", workerID, bpRes.RequestID)
		}
	}
}

// extractLinksBpSocket: BpResponseã‹ã‚‰HTMLãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
func extractLinksBpSocket(bpRes BpResponse, baseURLStr string) []string {
	var links []string

	if !strings.HasPrefix(bpRes.ContentType, "text/html") {
		return links
	}

	bodyBytes, err := base64.StdEncoding.DecodeString(bpRes.Body)
	if err != nil {
		log.Printf("âš ï¸  Base64 decode error: %v", err)
		return links
	}
	htmlContent := string(bodyBytes)

	baseURL, err := url.Parse(baseURLStr)
	if err != nil {
		log.Printf("âš ï¸  URL parse error: %v", err)
		return links
	}

	matches := linkRegex.FindAllStringSubmatch(htmlContent, -1)
	for _, match := range matches {
		if len(match) > 1 {
			relativeURL := match[1]
			resolvedURL, err := baseURL.Parse(relativeURL)

			if err == nil && resolvedURL.Host == baseURL.Host && resolvedURL.Scheme != "" {
				resolvedURL.RawQuery = ""
				resolvedURL.Fragment = ""
				links = append(links, resolvedURL.String())
			}
		}
	}
	return links
}
